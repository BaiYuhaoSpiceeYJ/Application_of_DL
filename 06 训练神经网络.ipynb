{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_path DATA_PATH]\n",
      "                             [--load_file LOAD_FILE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\TH\\AppData\\Roaming\\jupyter\\runtime\\kernel-67280004-c5f0-4137-a1eb-26e118f2230d.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "训练神经网络模型\n",
    "\n",
    "大家之后可以加上各种的 name_scope（命名空间）\n",
    "用 TensorBoard 来可视化\n",
    "\n",
    "==== 一些术语的概念 ====\n",
    "# Batch size : 批次(样本)数目。一次迭代（Forword 运算（用于得到损失函数）以及 BackPropagation 运算（用于更新神经网络参数））所用的样本数目。Batch size 越大，所需的内存就越大\n",
    "# Iteration : 迭代。每一次迭代更新一次权重（网络参数），每一次权重更新需要 Batch size 个数据进行 Forward 运算，再进行 BP 运算\n",
    "# Epoch : 纪元/时代。所有的训练样本完成一次迭代\n",
    "\n",
    "# 假如 : 训练集有 1000 个样本，Batch_size=10\n",
    "# 那么 : 训练完整个样本集需要： 100 次 Iteration，1 个 Epoch\n",
    "# 但一般我们都不止训练一个 Epoch\n",
    "\n",
    "==== 超参数（Hyper parameter）====\n",
    "init_scale : 权重参数（Weights）的初始取值跨度，一开始取小一些比较利于训练\n",
    "learning_rate : 学习率，训练时初始为 1.0\n",
    "num_layers : LSTM 层的数目（默认是 2）\n",
    "num_steps : LSTM 展开的步（step）数，相当于每个批次输入单词的数目（默认是 35）\n",
    "hidden_size : LSTM 层的神经元数目，也是词向量的维度（默认是 650）\n",
    "max_lr_epoch : 用初始学习率训练的 Epoch 数目（默认是 10）\n",
    "dropout : 在 Dropout 层的留存率（默认是 0.5）\n",
    "lr_decay : 在过了 max_lr_epoch 之后每一个 Epoch 的学习率的衰减率，训练时初始为 0.93。让学习率逐渐衰减是提高训练效率的有效方法\n",
    "batch_size : 批次(样本)数目。一次迭代（Forword 运算（用于得到损失函数）以及 BackPropagation 运算（用于更新神经网络参数））所用的样本数目\n",
    "（batch_size 默认是 20。取比较小的 batch_size 更有利于 Stochastic Gradient Descent（随机梯度下降），防止被困在局部最小值）\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(r'.\\RNN')\n",
    "#sys.path.append(r'.05 network')\n",
    "from utils import *\n",
    "from network import *\n",
    "\n",
    "\n",
    "def train(train_data, vocab_size, num_layers, num_epochs, batch_size, model_save_name,\n",
    "          learning_rate=1.0, max_lr_epoch=10, lr_decay=0.93, print_iter=50):\n",
    "    # 训练的输入\n",
    "    training_input = Input(batch_size=batch_size, num_steps=35, data=train_data)\n",
    "\n",
    "    # 创建训练的模型\n",
    "    m = Model(training_input, is_training=True, hidden_size=650, vocab_size=vocab_size, num_layers=num_layers)\n",
    "\n",
    "    # 初始化变量的操作\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # 初始的学习率（learning rate）的衰减率\n",
    "    orig_decay = lr_decay\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)  # 初始化所有变量\n",
    "\n",
    "        # Coordinator（协调器），用于协调线程的运行\n",
    "        coord = tf.train.Coordinator()\n",
    "        # 启动线程\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        # 为了用 Saver 来保存模型的变量\n",
    "        saver = tf.train.Saver() # max_to_keep 默认是 5, 只保存最近的 5 个模型参数文件\n",
    "\n",
    "        # 开始 Epoch 的训练\n",
    "        for epoch in range(num_epochs):\n",
    "            # 只有 Epoch 数大于 max_lr_epoch（设置为 10）后，才会使学习率衰减\n",
    "            # 也就是说前 10 个 Epoch 的学习率一直是 1, 之后每个 Epoch 学习率都会衰减\n",
    "            new_lr_decay = orig_decay ** max(epoch + 1 - max_lr_epoch, 0)\n",
    "            m.assign_lr(sess, learning_rate * new_lr_decay)\n",
    "\n",
    "            # 当前的状态\n",
    "            # 第二维是 2 是因为对每一个 LSTM 单元有两个来自上一单元的输入：\n",
    "            # 一个是 前一时刻 LSTM 的输出 h(t-1)\n",
    "            # 一个是 前一时刻的单元状态 C(t-1)\n",
    "            current_state = np.zeros((num_layers, 2, batch_size, m.hidden_size))\n",
    "\n",
    "            # 获取当前时间，以便打印日志时用\n",
    "            curr_time = datetime.datetime.now()\n",
    "\n",
    "            for step in range(training_input.epoch_size):\n",
    "                # train_op 操作：计算被修剪（clipping）过的梯度，并最小化 cost（误差）\n",
    "                # state 操作：返回时间维度上展开的最后 LSTM 单元的输出（C(t) 和 h(t)），作为下一个 Batch 的输入状态\n",
    "                if step % print_iter != 0:\n",
    "                    cost, _, current_state = sess.run([m.cost, m.train_op, m.state], feed_dict={m.init_state: current_state})\n",
    "                else:\n",
    "                    seconds = (float((datetime.datetime.now() - curr_time).seconds) / print_iter)#每50步算一次时间\n",
    "                    curr_time = datetime.datetime.now()\n",
    "                    cost, _, current_state, acc = sess.run([m.cost, m.train_op, m.state, m.accuracy], feed_dict={m.init_state: current_state})\n",
    "                    # 每 print_iter（默认是 50）打印当下的 Cost（误差/损失）和 Accuracy（精度）\n",
    "                    print(\"Epoch {}, 第 {} 步, 损失: {:.3f}, 精度: {:.3f}, 每步所用秒数: {:.2f}\".format(epoch, step, cost, acc, seconds))\n",
    "\n",
    "            # 保存一个模型的变量的 checkpoint 文件\n",
    "            saver.save(sess, save_path + '\\\\' + model_save_name, global_step=epoch)\n",
    "        # 对模型做一次总的保存\n",
    "        saver.save(sess, save_path + '\\\\' + model_save_name + '-final')\n",
    "\n",
    "        # 关闭线程\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if args.data_path:\n",
    "        data_path = args.data_path\n",
    "    train_data, valid_data, test_data, vocab_size, id_to_word = load_data(data_path)\n",
    "\n",
    "    train(train_data, vocab_size, num_layers=2, num_epochs=70, batch_size=20,\n",
    "          model_save_name='train-checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
