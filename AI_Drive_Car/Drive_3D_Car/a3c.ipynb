{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "A3C 算法。做决策的部分，相当于机器人的大脑\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import threading\n",
    "import scipy.signal\n",
    "import six.moves.queue as queue\n",
    "from collections import namedtuple\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# 用折扣因子 gamma 来衰减 x\n",
    "def discount(x, gamma):\n",
    "    return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "# 处理样本\n",
    "def process_rollout(rollout, gamma, lambda_=1.0):\n",
    "    \"\"\"\n",
    "    给一个样本，计算它的返回和优势\n",
    "    \"\"\"\n",
    "    batch_si = np.asarray(rollout.states)\n",
    "    batch_a = np.asarray(rollout.actions)\n",
    "    rewards = np.asarray(rollout.rewards)\n",
    "    vpred_t = np.asarray(rollout.values + [rollout.r])\n",
    "\n",
    "    rewards_plus_v = np.asarray(rollout.rewards + [rollout.r])\n",
    "    batch_r = discount(rewards_plus_v, gamma)[:-1]\n",
    "    delta_t = rewards + gamma * vpred_t[1:] - vpred_t[:-1]\n",
    "    # 计算 Advantage（优势）的公式来自这篇论文：\n",
    "    # https://arxiv.org/abs/1506.02438\n",
    "    batch_adv = discount(delta_t, gamma * lambda_)\n",
    "\n",
    "    features = rollout.features[0]\n",
    "    return Batch(batch_si, batch_a, batch_adv, batch_r, rollout.terminal, features)\n",
    "\n",
    "\n",
    "# 一个样本的命名 tuple（元组）\n",
    "Batch = namedtuple(\"Batch\", [\"si\", \"a\", \"adv\", \"r\", \"terminal\", \"features\"])\n",
    "\n",
    "\n",
    "class PartialRollout(object):\n",
    "    \"\"\"\n",
    "    一个完整样本的一部分。\n",
    "    如果一个 agent（智能体）已经运行了足够多的步数，\n",
    "    我们就处理它的经验\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.r = 0.0\n",
    "        self.terminal = False\n",
    "        self.features = []\n",
    "\n",
    "    def add(self, state, action, reward, value, terminal, features):\n",
    "        self.states += [state]\n",
    "        self.actions += [action]\n",
    "        self.rewards += [reward]\n",
    "        self.values += [value]\n",
    "        self.terminal = terminal\n",
    "        self.features += [features]\n",
    "\n",
    "    def extend(self, other):\n",
    "        assert not self.terminal\n",
    "        self.states.extend(other.states)\n",
    "        self.actions.extend(other.actions)\n",
    "        self.rewards.extend(other.rewards)\n",
    "        self.values.extend(other.values)\n",
    "        self.r = other.r\n",
    "        self.terminal = other.terminal\n",
    "        self.features.extend(other.features)\n",
    "\n",
    "\n",
    "class RunnerThread(threading.Thread):\n",
    "    \"\"\"\n",
    "    Universe 的环境和普通环境最大不同之一是：\n",
    "    Universe 的环境是 Real Time（实时）的，也就需要有一个\n",
    "    线程来与其交互，告诉它该干什么。RunnerThread 线程就是来做这事的\n",
    "    \"\"\"\n",
    "    def __init__(self, env, policy, num_local_steps, visualise):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.queue = queue.Queue(5)\n",
    "        self.num_local_steps = num_local_steps\n",
    "        self.env = env\n",
    "        self.last_features = None\n",
    "        self.policy = policy\n",
    "        self.daemon = True\n",
    "        self.sess = None\n",
    "        self.summary_writer = None\n",
    "        self.visualise = visualise\n",
    "\n",
    "    def start_runner(self, sess, summary_writer):\n",
    "        self.sess = sess\n",
    "        self.summary_writer = summary_writer\n",
    "        self.start()\n",
    "\n",
    "    def run(self):\n",
    "        with self.sess.as_default():\n",
    "            self._run()\n",
    "\n",
    "    def _run(self):\n",
    "        rollout_provider = env_runner(self.env, self.policy, self.num_local_steps, self.summary_writer, self.visualise)\n",
    "        while True:\n",
    "            # timeout 是凭经验设定的。\n",
    "            # 如果一个 worker 挂了，其他 worker 不会和它一起挂\n",
    "            self.queue.put(next(rollout_provider), timeout=600.0)\n",
    "\n",
    "\n",
    "def env_runner(env, policy, num_local_steps, summary_writer, render):\n",
    "    \"\"\"\n",
    "    RunnerThread 的主逻辑。\n",
    "    简单来说，它不停执行 Policy（策略），\n",
    "    只要样本长度超过了一定值，就会把 Policy 插入到队列中\n",
    "    \"\"\"\n",
    "    last_state = env.reset()\n",
    "    last_features = policy.get_initial_features()\n",
    "    length = 0\n",
    "    rewards = 0\n",
    "\n",
    "    while True:\n",
    "        terminal_end = False\n",
    "        rollout = PartialRollout()\n",
    "\n",
    "        for _ in range(num_local_steps):\n",
    "            fetched = policy.act(last_state, *last_features)\n",
    "            action, value_, features = fetched[0], fetched[1], fetched[2:]\n",
    "            # 用 argmax 方法从 One-Hot 编码转换出所需结果\n",
    "            state, reward, terminal, info = env.step(action.argmax())\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # 收集经验\n",
    "            rollout.add(last_state, action, reward, value_, terminal, last_features)\n",
    "            length += 1\n",
    "            rewards += reward\n",
    "\n",
    "            last_state = state\n",
    "            last_features = features\n",
    "\n",
    "            if info:\n",
    "                summary = tf.Summary()\n",
    "                for k, v in info.items():\n",
    "                    summary.value.add(tag=k, simple_value=float(v))\n",
    "                summary_writer.add_summary(summary, policy.global_step.eval())\n",
    "                summary_writer.flush()\n",
    "\n",
    "            timestep_limit = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "            if terminal or length >= timestep_limit:\n",
    "                terminal_end = True\n",
    "                if length >= timestep_limit or not env.metadata.get('semantics.autoreset'):\n",
    "                    last_state = env.reset()\n",
    "                last_features = policy.get_initial_features()\n",
    "                print(\"回合结束. 回合奖励: {}. 回合长度: {}\".format(rewards, length))\n",
    "                length = 0\n",
    "                rewards = 0\n",
    "                break\n",
    "\n",
    "        if not terminal_end:\n",
    "            rollout.r = policy.value(last_state, *last_features)\n",
    "\n",
    "        # 一旦收集了足够的经验，就把它发送出去，并且让 RunnerThread 把它置于队列中\n",
    "        yield rollout\n",
    "\n",
    "\n",
    "# A3C 算法的一个实现，针对 VNC 的环境做了优化和适配\n",
    "class A3C(object):\n",
    "    def __init__(self, env, task, visualise):\n",
    "        self.env = env\n",
    "        self.task = task\n",
    "        worker_device = \"/job:worker/task:{}/cpu:0\".format(task)\n",
    "        with tf.device(tf.train.replica_device_setter(1, worker_device=worker_device)):\n",
    "            with tf.variable_scope(\"global\"):\n",
    "                self.network = LSTMPolicy(env.observation_space.shape, env.action_space.n)\n",
    "                self.global_step = tf.get_variable(\"global_step\", [], tf.int32, initializer=tf.constant_initializer(0, dtype=tf.int32),\n",
    "                                                   trainable=False)\n",
    "\n",
    "        with tf.device(worker_device):\n",
    "            with tf.variable_scope(\"local\"):\n",
    "                self.local_network = pi = LSTMPolicy(env.observation_space.shape, env.action_space.n)\n",
    "                pi.global_step = self.global_step\n",
    "\n",
    "            self.ac = tf.placeholder(tf.float32, [None, env.action_space.n], name=\"ac\")\n",
    "            self.adv = tf.placeholder(tf.float32, [None], name=\"adv\")\n",
    "            self.r = tf.placeholder(tf.float32, [None], name=\"r\")\n",
    "\n",
    "            log_prob_tf = tf.nn.log_softmax(pi.logits)\n",
    "            prob_tf = tf.nn.softmax(pi.logits)\n",
    "\n",
    "            # Policy Gradient 的所谓 loss（损失）：\n",
    "            # 它的导数恰好就是 Policy Gradient\n",
    "            # self.adv 用于存储 Advantage（优势），\n",
    "            # Advantage 是在 process_rollout 方法中算出来的\n",
    "            pi_loss = - tf.reduce_sum(tf.reduce_sum(log_prob_tf * self.ac, [1]) * self.adv)\n",
    "\n",
    "            # 值函数的 loss（损失）\n",
    "            vf_loss = 0.5 * tf.reduce_sum(tf.square(pi.vf - self.r))\n",
    "            entropy = - tf.reduce_sum(prob_tf * log_prob_tf)\n",
    "\n",
    "            bs = tf.to_float(tf.shape(pi.x)[0])\n",
    "            self.loss = pi_loss + 0.5 * vf_loss - entropy * 0.01\n",
    "\n",
    "            # 20 代表 \"本地步数\"，也就是在更新参数前我们运行策略的时间步数\n",
    "            # \"本地步数\" 越大的话，一方面，Policy Gradient 估计的方差就越小\n",
    "            # 另一方面，我们更新参数的频率就降低了，会减慢学习速度\n",
    "            # 20 是一个比较好的超参数。\n",
    "            self.runner = RunnerThread(env, pi, 20, visualise)\n",
    "\n",
    "            grads = tf.gradients(self.loss, pi.var_list)\n",
    "\n",
    "            # 为了用 TensorBoard 看到这些信息\n",
    "            tf.summary.scalar(\"model/policy_loss\", pi_loss / bs)\n",
    "            tf.summary.scalar(\"model/value_loss\", vf_loss / bs)\n",
    "            tf.summary.scalar(\"model/entropy\", entropy / bs)\n",
    "            tf.summary.image(\"model/state\", pi.x)\n",
    "            tf.summary.scalar(\"model/grad_global_norm\", tf.global_norm(grads))\n",
    "            tf.summary.scalar(\"model/var_global_norm\", tf.global_norm(pi.var_list))\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "            grads, _ = tf.clip_by_global_norm(grads, 40.0)\n",
    "\n",
    "            # 从 ps（参数服务器）将参数拷贝到 worker 的神经网络模型中\n",
    "            self.sync = tf.group(*[v1.assign(v2) for v1, v2 in zip(pi.var_list, self.network.var_list)])\n",
    "\n",
    "            grads_and_vars = list(zip(grads, self.network.var_list))\n",
    "            inc_step = self.global_step.assign_add(tf.shape(pi.x)[0])\n",
    "\n",
    "            # 每一个 worker 有一个不同的 Adam Optimizer（Adam 优化器）集\n",
    "            opt = tf.train.AdamOptimizer(1e-4)\n",
    "            self.train_op = tf.group(opt.apply_gradients(grads_and_vars), inc_step)\n",
    "            self.summary_writer = None\n",
    "            self.local_steps = 0\n",
    "\n",
    "    def start(self, sess, summary_writer):\n",
    "        self.runner.start_runner(sess, summary_writer)\n",
    "        self.summary_writer = summary_writer\n",
    "\n",
    "    # 从 RunnerThread 的队列里取出一个 batch（样本）\n",
    "    def pull_batch_from_queue(self):\n",
    "        rollout = self.runner.queue.get(timeout=600.0)\n",
    "        while not rollout.terminal:\n",
    "            try:\n",
    "                rollout.extend(self.runner.queue.get_nowait())\n",
    "            except queue.Empty:\n",
    "                break\n",
    "        return rollout\n",
    "\n",
    "    def process(self, sess):\n",
    "        \"\"\"\n",
    "        抓取 RunnerThread 产生的一个样本，更新它们的参数\n",
    "        更新的参数会被发送到 ps（参数服务器）\n",
    "        \"\"\"\n",
    "        sess.run(self.sync)  # 将参数从 ps（参数服务器）拷贝到本地 worker\n",
    "        rollout = self.pull_batch_from_queue()\n",
    "        batch = process_rollout(rollout, gamma=0.99, lambda_=1.0)\n",
    "\n",
    "        should_compute_summary = self.task == 0 and self.local_steps % 11 == 0\n",
    "\n",
    "        if should_compute_summary:\n",
    "            fetches = [self.summary_op, self.train_op, self.global_step]\n",
    "        else:\n",
    "            fetches = [self.train_op, self.global_step]\n",
    "\n",
    "        feed_dict = {\n",
    "            self.local_network.x: batch.si,\n",
    "            self.ac: batch.a,\n",
    "            self.adv: batch.adv,\n",
    "            self.r: batch.r,\n",
    "            self.local_network.state_in[0]: batch.features[0],\n",
    "            self.local_network.state_in[1]: batch.features[1],\n",
    "        }\n",
    "\n",
    "        fetched = sess.run(fetches, feed_dict=feed_dict)\n",
    "\n",
    "        if should_compute_summary:\n",
    "            self.summary_writer.add_summary(tf.Summary.FromString(fetched[0]), fetched[-1])\n",
    "            self.summary_writer.flush()\n",
    "        self.local_steps += 1\n",
    "\n",
    "\n",
    "# 用于变量的初始化\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer\n",
    "\n",
    "\n",
    "# 扁平层\n",
    "def flatten(x):\n",
    "    return tf.reshape(x, [-1, np.prod(x.get_shape().as_list()[1:])])\n",
    "\n",
    "\n",
    "# 二维卷积层\n",
    "def conv2d(x, num_filters, name, filter_size=(3, 3), stride=(1, 1), pad=\"SAME\", dtype=tf.float32, collections=None):\n",
    "    with tf.variable_scope(name):\n",
    "        stride_shape = [1, stride[0], stride[1], 1]\n",
    "        filter_shape = [filter_size[0], filter_size[1], int(x.get_shape()[3]), num_filters]\n",
    "\n",
    "        fan_in = np.prod(filter_shape[:3])\n",
    "        fan_out = np.prod(filter_shape[:2]) * num_filters\n",
    "        # 初始化 weight（权重）\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "\n",
    "        w = tf.get_variable(\"W\", filter_shape, dtype, tf.random_uniform_initializer(-w_bound, w_bound),\n",
    "                            collections=collections)\n",
    "        b = tf.get_variable(\"b\", [1, 1, 1, num_filters], initializer=tf.constant_initializer(0.0),\n",
    "                            collections=collections)\n",
    "        return tf.nn.conv2d(x, w, stride_shape, pad) + b\n",
    "\n",
    "\n",
    "# 线性计算：y = W * x + b\n",
    "def linear(x, size, name, initializer=None, bias_init=0):\n",
    "    w = tf.get_variable(name + \"/w\", [x.get_shape()[1], size], initializer=initializer)\n",
    "    b = tf.get_variable(name + \"/b\", [size], initializer=tf.constant_initializer(bias_init))\n",
    "    return tf.matmul(x, w) + b\n",
    "\n",
    "\n",
    "def categorical_sample(logits, d):\n",
    "    value = tf.squeeze(tf.multinomial(logits - tf.reduce_max(logits, [1], keep_dims=True), 1), [1])\n",
    "    return tf.one_hot(value, d)\n",
    "\n",
    "\n",
    "# 用 LSTM (RNN) 循环神经网络来做策略梯度\n",
    "class LSTMPolicy(object):\n",
    "    def __init__(self, ob_space, ac_space):\n",
    "        self.x = x = tf.placeholder(tf.float32, [None] + list(ob_space))\n",
    "\n",
    "        for i in range(4):\n",
    "            x = tf.nn.elu(conv2d(x, 32, \"l{}\".format(i + 1), [3, 3], [2, 2]))\n",
    "        # 在扁平化之后引入一个假的样本维度（1），这样我们就可以在时间维度上做 LSTM 操作\n",
    "        x = tf.expand_dims(flatten(x), [0])\n",
    "\n",
    "        size = 256\n",
    "\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(size, state_is_tuple=True)\n",
    "\n",
    "        self.state_size = lstm.state_size\n",
    "        step_size = tf.shape(self.x)[:1]\n",
    "\n",
    "        c_init = np.zeros((1, lstm.state_size.c), np.float32)\n",
    "        h_init = np.zeros((1, lstm.state_size.h), np.float32)\n",
    "        self.state_init = [c_init, h_init]\n",
    "        c_in = tf.placeholder(tf.float32, [1, lstm.state_size.c])\n",
    "        h_in = tf.placeholder(tf.float32, [1, lstm.state_size.h])\n",
    "        self.state_in = [c_in, h_in]\n",
    "\n",
    "        state_in = tf.contrib.rnn.LSTMStateTuple(c_in, h_in)\n",
    "\n",
    "        lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n",
    "            lstm, x, initial_state=state_in, sequence_length=step_size,\n",
    "            time_major=False)\n",
    "        lstm_c, lstm_h = lstm_state\n",
    "        x = tf.reshape(lstm_outputs, [-1, size])\n",
    "        self.logits = linear(x, ac_space, \"action\", normalized_columns_initializer(0.01))\n",
    "        self.vf = tf.reshape(linear(x, 1, \"value\", normalized_columns_initializer(1.0)), [-1])\n",
    "        self.state_out = [lstm_c[:1, :], lstm_h[:1, :]]\n",
    "        self.sample = categorical_sample(self.logits, ac_space)[0, :]\n",
    "        self.var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n",
    "\n",
    "    def get_initial_features(self):\n",
    "        return self.state_init\n",
    "\n",
    "    # 实施 action\n",
    "    def act(self, ob, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run([self.sample, self.vf] + self.state_out,\n",
    "                        {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})\n",
    "\n",
    "    # 计算 Q 值\n",
    "    def value(self, ob, c, h):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run(self.vf, {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
