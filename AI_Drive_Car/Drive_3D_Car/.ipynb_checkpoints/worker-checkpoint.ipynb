{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "分布式集群（Cluster）的创建和配置，等\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import time\n",
    "import signal\n",
    "import logging\n",
    "import argparse\n",
    "import go_vncdriver\n",
    "import tensorflow as tf\n",
    "\n",
    "from a3c import A3C\n",
    "from env import create_env\n",
    "\n",
    "# 配置日志系统\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# 设置 write_meta_graph 为 False（不保存），因为很耗时，可能会拖慢训练\n",
    "class FastSaver(tf.train.Saver):\n",
    "    def save(self, sess, save_path, global_step=None, latest_filename=None, meta_graph_suffix=\"meta\", write_meta_graph=True):\n",
    "        super(FastSaver, self).save(sess, save_path, global_step, latest_filename, meta_graph_suffix, False)\n",
    "\n",
    "\n",
    "# 如果是 worker 的话，所运行的步骤\n",
    "def run(args, server):\n",
    "    env = create_env(client_id=str(args.task), remotes=args.remotes)\n",
    "    trainer = A3C(env, args.task, args.visualise)\n",
    "\n",
    "    # 以 'local' 开头的变量（局部变量）不会被保存在 checkpoint 参数文件中\n",
    "    variables_to_save = [v for v in tf.global_variables() if not v.name.startswith(\"local\")]\n",
    "    init_op = tf.variables_initializer(variables_to_save)\n",
    "    init_all_op = tf.global_variables_initializer()\n",
    "\n",
    "    # 保存变量到参数文件中\n",
    "    saver = FastSaver(variables_to_save)\n",
    "\n",
    "    # 获取可被训练的变量\n",
    "    var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n",
    "\n",
    "    logger.info('可被训练的变量 :')\n",
    "    for v in var_list:\n",
    "        logger.info('  %s %s', v.name, v.get_shape())\n",
    "\n",
    "    def init_fn(ses):\n",
    "        logger.info(\"初始化所有参数。\")\n",
    "        ses.run(init_all_op)\n",
    "\n",
    "    config = tf.ConfigProto(device_filters=[\"/job:ps\", \"/job:worker/task:{}/cpu:0\".format(args.task)])\n",
    "\n",
    "    logdir = os.path.join(args.log_dir, 'train')\n",
    "    # 写入 TensorBoard 的日志文件\n",
    "    summary_writer = tf.summary.FileWriter(logdir + \"_%d\" % args.task)\n",
    "\n",
    "    logger.info(\"存储 TensorBoard 文件的目录: %s_%s\", logdir, args.task)\n",
    "\n",
    "    # 一个高层的 Wrapper（包装类）\n",
    "    # 可以做 TensorBoard 日志文件的保存，参数文件的保存，等等操作\n",
    "    sv = tf.train.Supervisor(is_chief=(args.task == 0),\n",
    "                             logdir=logdir,  # 存储参数文件的目录\n",
    "                             saver=saver,    # 存储参数文件所用的 Saver\n",
    "                             summary_op=None,\n",
    "                             init_op=init_op,\n",
    "                             init_fn=init_fn,\n",
    "                             summary_writer=summary_writer,  # 存储 TensorBoard 日志文件的 FileWriter\n",
    "                             ready_op=tf.report_uninitialized_variables(variables_to_save),\n",
    "                             global_step=trainer.global_step,\n",
    "                             save_model_secs=30,\n",
    "                             save_summaries_secs=30)\n",
    "\n",
    "    # 总的可运行步数。可修改\n",
    "    num_global_steps = 100000000\n",
    "\n",
    "    logger.info(\"启动会话中...\")\n",
    "    with sv.managed_session(server.target, config=config) as sess, sess.as_default():\n",
    "        sess.run(trainer.sync)\n",
    "        trainer.start(sess, summary_writer)\n",
    "        global_step = sess.run(trainer.global_step)\n",
    "        logger.info(\"在第 %d 步开始训练\", global_step)\n",
    "        while not sv.should_stop() and (not num_global_steps or global_step < num_global_steps):\n",
    "            trainer.process(sess)\n",
    "            global_step = sess.run(trainer.global_step)\n",
    "\n",
    "    # 停止所有服务\n",
    "    sv.stop()\n",
    "    logger.info('已经 %s 步了. worker 被停止.', global_step)\n",
    "\n",
    "\n",
    "# 集群（Cluster）的配置\n",
    "def cluster_spec(num_workers, num_ps):\n",
    "    cluster = {}\n",
    "    port = 12222\n",
    "\n",
    "    # 所有 ps（参数服务器）的配置\n",
    "    all_ps = []\n",
    "    host = '127.0.0.1'\n",
    "    for _ in range(num_ps):\n",
    "        all_ps.append('{}:{}'.format(host, port))\n",
    "        port += 1\n",
    "    cluster['ps'] = all_ps\n",
    "\n",
    "    # 所有 worker 的配置\n",
    "    all_workers = []\n",
    "    for _ in range(num_workers):\n",
    "        all_workers.append('{}:{}'.format(host, port))\n",
    "        port += 1\n",
    "    cluster['worker'] = all_workers\n",
    "\n",
    "    return cluster\n",
    "\n",
    "\n",
    "# 主函数\n",
    "def main(_):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-v', '--verbose', action='count', dest='verbosity', default=0, help='设置冗余程度')\n",
    "    parser.add_argument('--task', default=0, type=int, help='task 下标')\n",
    "    parser.add_argument('--job-name', default=\"worker\", help='Job 是 worker 还是 ps')\n",
    "    parser.add_argument('--num-workers', default=1, type=int, help='worker 的数目')\n",
    "    parser.add_argument('--log-dir', default=\"neonrace\", help='包含日志文件和各种 checkpoint（检查点）文件的总目录')\n",
    "    parser.add_argument('-r', '--remotes', default=None,\n",
    "                        help='远程 VNC 服务器的地址 (例如：-r vnc://localhost:5900+15900, vnc://localhost:5901+15901).')\n",
    "    parser.add_argument('--visualise', action='store_true',\n",
    "                        help='可视化与否：如果设置，那么在每个时间步之间用 env.render() 来渲染（显示）游戏环境')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    spec = cluster_spec(args.num_workers, 1)\n",
    "    cluster = tf.train.ClusterSpec(spec).as_cluster_def()\n",
    "\n",
    "    # 定义终止线程的信号\n",
    "    def shutdown(signal):\n",
    "        logger.warn('收到信号 %s: 退出中...', signal)\n",
    "        sys.exit(128+signal)\n",
    "    # hang-up 信号，例如用户注销（logout）\n",
    "    # 或者 网络断开时（hang-up 信号 会被 nohup 模式忽略）\n",
    "    signal.signal(signal.SIGHUP, shutdown)\n",
    "    # interrupt 信号，例如 Ctrl + C 组合键\n",
    "    signal.signal(signal.SIGINT, shutdown)\n",
    "    # terminal 信号。例如 kill 命令\n",
    "    signal.signal(signal.SIGTERM, shutdown)\n",
    "\n",
    "    # 如果 Job 是 worker\n",
    "    if args.job_name == \"worker\":\n",
    "        server = tf.train.Server(cluster, job_name=\"worker\", task_index=args.task, config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=2))\n",
    "        run(args, server)\n",
    "    # 如果 Job 是 ps（Parameter Server）\n",
    "    else:\n",
    "        tf.train.Server(cluster, job_name=\"ps\", task_index=args.task, config=tf.ConfigProto(device_filters=[\"/job:ps\"]))\n",
    "        while True:\n",
    "            time.sleep(1000)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 运行 main()\n",
    "    tf.app.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
