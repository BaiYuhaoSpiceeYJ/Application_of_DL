{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "游戏的主程序，调用机器人的 Policy Gradient 决策大脑\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "\n",
    "from policy_gradient import PolicyGradient\n",
    "\n",
    "\n",
    "# 伪随机数。为了能够复现结果\n",
    "np.random.seed(1)\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped    # 取消限制\n",
    "env.seed(1)   # 普通的 Policy Gradient 方法, 回合的方差比较大, 所以选一个好点的随机种子\n",
    "\n",
    "print(env.action_space)            # 查看这个环境中可用的 action 有多少个\n",
    "print(env.observation_space)       # 查看这个环境中 state/observation 有多少个特征值\n",
    "print(env.observation_space.high)  # 查看 observation 最高取值\n",
    "print(env.observation_space.low)   # 查看 observation 最低取值\n",
    "\n",
    "update_frequency = 5   # 更新频率，多少回合更新一次\n",
    "total_episodes = 3000  # 总回合数\n",
    "\n",
    "# 创建 PolicyGradient 对象\n",
    "agent = PolicyGradient(lr=0.01,\n",
    "                       a_size=env.action_space.n,   # 对 CartPole-v0 是 2, 两个 action，向左/向右\n",
    "                       s_size=env.observation_space.shape[0],  # 对 CartPole-v0 是 4\n",
    "                       h_size=8)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 初始化所有全局变量\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 总的奖励\n",
    "    total_reward = []\n",
    "\n",
    "    gradient_buffer = sess.run(tf.trainable_variables())\n",
    "    for index, grad in enumerate(gradient_buffer):\n",
    "        gradient_buffer[index] = grad * 0\n",
    "\n",
    "    i = 0  # 第几回合\n",
    "    while i < total_episodes:\n",
    "        # 初始化 state（状态）\n",
    "        s = env.reset()\n",
    "        \n",
    "        episode_reward = 0\n",
    "        episode_history = []\n",
    "\n",
    "        while True:\n",
    "            # 更新可视化环境\n",
    "            env.render()\n",
    "            \n",
    "            # 根据神经网络的输出，随机挑选 action\n",
    "            a_dist = sess.run(agent.output, feed_dict={agent.state_in: [s]})\n",
    "            a = np.random.choice(a_dist[0], p=a_dist[0])\n",
    "            a = np.argmax(a_dist == a)\n",
    "\n",
    "            # 实施这个 action, 并得到环境返回的下一个 state, reward 和 done(本回合是否结束)\n",
    "            s_, r, done, _ = env.step(a)  # 这里的 r（奖励）不能准确引导学习\n",
    "\n",
    "            x, x_dot, theta, theta_dot = s_  # 把 s_ 细分开, 为了修改原配的 reward\n",
    "\n",
    "            # x 是车的水平位移。所以 r1 是车越偏离中心, 分越少\n",
    "            # theta 是棒子离垂直的角度, 角度越大, 越不垂直。所以 r2 是棒越垂直, 分越高\n",
    "            r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
    "            r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
    "            r = r1 + r2  # 总 reward 是 r1 和 r2 的结合, 既考虑位置, 也考虑角度, 这样学习更有效率\n",
    "\n",
    "            episode_history.append([s, a, r, s_])\n",
    "\n",
    "            episode_reward += r\n",
    "            s = s_\n",
    "\n",
    "            # Policy Gradient 是回合更新\n",
    "            if done:  # 如果此回合结束\n",
    "                # 更新神经网络\n",
    "                episode_history = np.array(episode_history)\n",
    "                \n",
    "                episode_history[:, 2] = agent.discount_rewards(episode_history[:, 2])\n",
    "                \n",
    "                feed_dict = {\n",
    "                    agent.reward_holder: episode_history[:, 2],\n",
    "                    agent.action_holder: episode_history[:, 1],\n",
    "                    agent.state_in: np.vstack(episode_history[:, 0])\n",
    "                }\n",
    "\n",
    "                # 计算梯度\n",
    "                grads = sess.run(agent.gradients, feed_dict=feed_dict)\n",
    "                \n",
    "                for idx, grad in enumerate(grads):\n",
    "                    gradient_buffer[idx] += grad\n",
    "\n",
    "                if i % update_frequency == 0 and i != 0:\n",
    "                    feed_dict = dictionary = dict(zip(agent.gradient_holders, gradient_buffer))\n",
    "\n",
    "                    # 应用梯度下降来更新参数\n",
    "                    _ = sess.run(agent.update_batch, feed_dict=feed_dict)\n",
    "\n",
    "                    for index, grad in enumerate(gradient_buffer):\n",
    "                        gradient_buffer[index] = grad * 0\n",
    "\n",
    "                total_reward.append(episode_reward)\n",
    "                break\n",
    "\n",
    "        # 每 50 回合打印平均奖励\n",
    "        if i % 50 == 0:\n",
    "            print(\"回合 {} - {} 的平均奖励: {}\".format(i, i + 50, np.mean(total_reward[-50:])))\n",
    "\n",
    "        i += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
