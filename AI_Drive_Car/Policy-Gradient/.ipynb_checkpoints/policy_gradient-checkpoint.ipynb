{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Policy Gradient 算法（REINFORCE）。做决策的部分，相当于机器人的大脑\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    xrange = xrange  # Python 2\n",
    "except:\n",
    "    xrange = range   # Python 3\n",
    "\n",
    "\n",
    "# 策略梯度 类\n",
    "class PolicyGradient:\n",
    "    def __init__(self,\n",
    "                 lr,      # 学习速率\n",
    "                 s_size,  # state/observation 的特征数目\n",
    "                 a_size,  # action 的数目\n",
    "                 h_size,  # hidden layer（隐藏层）神经元数目\n",
    "                 discount_factor=0.99  # 折扣因子\n",
    "    ):\n",
    "        self.gamma = discount_factor  # Reward 递减率\n",
    "\n",
    "        # 神经网络的前向传播部分。大脑根据 state 来选 action\n",
    "        self.state_in = tf.placeholder(shape=[None, s_size], dtype=tf.float32)\n",
    "\n",
    "        # 第一层全连接层\n",
    "        hidden = tf.layers.dense(self.state_in, h_size, activation=tf.nn.relu)\n",
    "\n",
    "        # 第二层全连接层，用 Softmax 来算概率\n",
    "        self.output = tf.layers.dense(hidden, a_size, activation=tf.nn.softmax)\n",
    "\n",
    "        # 直接选择概率最大的那个 action\n",
    "        self.chosen_action = tf.argmax(self.output, 1)\n",
    "\n",
    "        # 下面主要是负责训练的一些过程\n",
    "        # 我们给神经网络传递 reward 和 action，为了计算 loss\n",
    "        # 再用 loss 来调节神经网络的参数\n",
    "        self.reward_holder = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "\n",
    "        self.indexes = tf.range(0, tf.shape(self.output)[0]) * tf.shape(self.output)[1] + self.action_holder\n",
    "        self.outputs = tf.gather(tf.reshape(self.output, [-1]), self.indexes)\n",
    "\n",
    "        # 计算 loss（和平时说的 loss 不一样）有一个负号\n",
    "        # 因为 TensorFlow 自带的梯度下降只能 minimize（最小化）loss\n",
    "        # 而 Policy Gradient 里面是要让这个所谓的 loss 最大化\n",
    "        # 因此需要反一下。对负的去让它最小化，就是让它正向最大化\n",
    "        self.loss = -tf.reduce_mean(tf.log(self.outputs) * self.reward_holder)\n",
    "\n",
    "        # 得到可被训练的变量\n",
    "        train_vars = tf.trainable_variables()\n",
    "        \n",
    "        self.gradient_holders = []\n",
    "        \n",
    "        for index, var in enumerate(train_vars):\n",
    "            placeholder = tf.placeholder(tf.float32, name=str(index) + '_holder')\n",
    "            self.gradient_holders.append(placeholder)\n",
    "\n",
    "        # 对 loss 以 train_vars 来计算梯度\n",
    "        self.gradients = tf.gradients(self.loss, train_vars)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        # apply_gradients 是 minimize 方法的第二部分，应用梯度\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders, train_vars))\n",
    "\n",
    "    # 计算折扣后的 reward\n",
    "    # 公式： E = r1 + r2 * gamma + r3 * gamma * gamma + r4 * gamma * gamma * gamma ...\n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_r = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        for t in reversed(xrange(0, rewards.size)):\n",
    "            running_add = running_add * self.gamma + rewards[t]\n",
    "            discounted_r[t] = running_add\n",
    "        return discounted_r\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
